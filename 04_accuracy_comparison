import pandas as pd
import numpy as np
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# --------------------------------------------------------------------------------
# --- Configuration Constants ---
# --------------------------------------------------------------------------------

CSV_FILE_NAME = 'sample_cleaned_data.csv'
SN_PRED_PATH = 'sn_predictions.pkl'
CHRONOS_PRED_PATH = 'chronos_forecasts.pkl'

# Unit conversion factors (for recalculating MASE base from raw data)
KWH_TO_MJ = 3.6
M3_TO_MJ = 45.0 

# Evaluation period (Test period)
EVAL_START_DATE = '2023-04'
EVAL_END_DATE = '2024-03'

TARGET_COLUMNS = [
    'PV_gene(MJ)', 
    'FC_gene(MJ)', 
    'elect_cons(MJ)', 
    'gas_cons(MJ)'
]

# --------------------------------------------------------------------------------
# --- 1. Data Loading and Merging ---
# --------------------------------------------------------------------------------

def load_data_and_merge(csv_file, sn_path, chronos_path):
    
    print("--- 1-1. Loading and Preprocessing Actual Data ---")
    try:
        # Load raw 30-min data (CSV) to prepare for monthly aggregation
        original_df = pd.read_csv(csv_file, parse_dates=['datetime'])
        original_df.set_index('datetime', inplace=True)
        
        # Replicate monthly/MJ preprocessing (extracting necessary columns)
        # Columns must be converted back to original units (kWh/m3) before sum()
        df = original_df[['ID'] + [col.replace('(MJ)', '(kWh)').replace('(m3)', '(m3)') for col in TARGET_COLUMNS] + ['temp', 'WNDSPD', 'RHUM', 'PRCRIN_30MIN', 'GLBRAD_30MIN']]
        
        # Group by ID and resample to monthly sum
        df_monthly = df.groupby('ID').resample('MS').sum().reset_index(level='ID')
        
        # Unit Conversion
        for col in [c for c in df_monthly.columns if '(kWh)' in c]:
            df_monthly[col] = df_monthly[col] * KWH_TO_MJ
        for col in [c for c in df_monthly.columns if '(m3)' in c]:
            df_monthly[col] = df_monthly[col] * M3_TO_MJ
        
        # Correct column names to MJ unit
        df_monthly.columns = [col.replace('(kWh)', '(MJ)').replace('(m3)', '(MJ)') for col in df_monthly.columns]
        
        # Convert timestamp column to 'YYYY-MM' string
        df_monthly.reset_index(names=['timestamp'], inplace=True)
        df_monthly['timestamp'] = df_monthly['timestamp'].dt.strftime('%Y-%m')
        
        # Extract actual values for the evaluation period only
        df_true = df_monthly[
            (df_monthly['timestamp'] >= EVAL_START_DATE) & (df_monthly['timestamp'] <= EVAL_END_DATE)
        ].copy()
        
        print(f"Actual Data Size (Evaluation Period): {df_true.shape}")
        
    except Exception as e:
        print(f"Error: An error occurred during actual data loading: {e}")
        return pd.DataFrame()


    # --- 1-2. Loading and Merging Forecast Results ---
    try:
        print(f"--- 1-2. Loading {SN_PRED_PATH} and {CHRONOS_PRED_PATH} ---")
        df_sn = pd.read_pickle(sn_path)
        df_chronos = pd.read_pickle(chronos_path)
    except FileNotFoundError as e:
        print(f"Error: Forecast files not found. Check if previous steps were executed: {e}")
        return pd.DataFrame()
    except Exception as e:
        print(f"Error: An error occurred during forecast file loading: {e}")
        return pd.DataFrame()

    # --- 1-3. Merging ---
    # Merge actual values and SN forecasts
    combined_df = pd.merge(df_true, df_sn, on=['ID', 'timestamp'], how='inner')
    
    # Merge Chronos forecasts (including quantiles)
    combined_df = pd.merge(combined_df, df_chronos, on=['ID', 'timestamp'], how='inner')
    
    # Verify the final evaluation DataFrame
    print(f"✅ Merging of forecasts and actual values completed. Data size: {combined_df.shape}")
    return combined_df

# --------------------------------------------------------------------------------
# --- 2. Evaluation Metric Calculation Helper Functions ---
# --------------------------------------------------------------------------------

def calculate_mae_and_rmse(y_true, y_pred):
    """Calculates MAE and RMSE."""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    return mae, rmse

def calculate_mase(y_true, y_pred, y_baseline_hist):
    """
    Calculates MASE (Mean Absolute Scaled Error).
    
    Args:
        y_true (np.array): Actual values (evaluation period)
        y_pred (np.array): Proposed model's forecasts (evaluation period)
        y_baseline_hist (np.array): Base for historical baseline error (e.g., 1-step lag error base)
        
    Returns:
        float: MASE value (MAE of proposed model / MAE of baseline model)
    """
    # Baseline Model: Uses Naive (1-month lag) MAE as the denominator
    # y_baseline_hist is used to calculate the denominator: mean(|y[t] - y[t-1]|)
    
    # 1. MAE of the Proposed Model (Numerator)
    mae_proposed = mean_absolute_error(y_true, y_pred)
    
    # 2. MAE of the Baseline Model (Denominator)
    # Baseline error: |y_baseline_hist[t] - y_baseline_hist[t-1]|
    # The calculation uses the historical series leading up to and including the evaluation period.
    
    # Absolute difference of consecutive historical actual values (1-step lag error)
    abs_diff_baseline = np.abs(y_baseline_hist[1:] - y_baseline_hist[:-1])
    # The average is calculated by dividing by the number of differences (N-1)
    mae_baseline = np.mean(abs_diff_baseline)
    
    # 3. MASE Calculation
    if mae_baseline == 0:
        return 999.0 # Avoid division by zero
    return mae_proposed / mae_baseline


# --------------------------------------------------------------------------------
# --- 3. Probabilistic Forecast Evaluation (PICP) ---
# --------------------------------------------------------------------------------

def calculate_picp(df, var_name, nominal_coverage=0.80):
    """
    Calculates PICP (Prediction Interval Coverage Probability).
    
    Args:
        df (pd.DataFrame): DataFrame containing actual values and quantile forecasts
        var_name (str): Target variable name (e.g., 'PV_gene(MJ)')
        nominal_coverage (float): Nominal coverage level (e.g., 0.80)
    """
    # Determine quantile column names (e.g., 0.80 coverage -> q10 and q90)
    lower_q = f"{var_name}_chronos_q{int((1 - nominal_coverage) / 2 * 100):02d}"  # e.g., q10
    upper_q = f"{var_name}_chronos_q{int((1 + nominal_coverage) / 2 * 100):02d}"  # e.g., q90
    
    # Check if required columns exist
    if not all(col in df.columns for col in [var_name, lower_q, upper_q]):
        print(f"Warning: Required quantile columns not found for {var_name}.")
        return None
        
    # Check if the actual value falls within the prediction interval (True/False)
    df['is_covered'] = (df[var_name] >= df[lower_q]) & (df[var_name] <= df[upper_q])
    
    # Calculate coverage rate (proportion of True)
    picp = df['is_covered'].mean()
    
    return picp

# --------------------------------------------------------------------------------
# --- 4. Main Process ---
# --------------------------------------------------------------------------------

def main():
    
    # Evaluation DataFrame combining actual values and all model forecasts
    combined_evaluation_df = load_data_and_merge(CSV_FILE_NAME, SN_PRED_PATH, CHRONOS_PRED_PATH)
    
    if combined_evaluation_df.empty:
        return

    # Prepare historical actual data for MASE calculation base
    # This requires the actual data from the period prior to the evaluation period (e.g., Mar 2022 to Mar 2024)
    
    # Reload full preprocessed data (necessary because load_data_and_merge only returns the evaluation period)
    df_raw_full = load_data_and_merge(CSV_FILE_NAME, SN_PRED_PATH, CHRONOS_PRED_PATH)
    
    # Data including the period before evaluation (for MASE denominator base)
    # '2022-03' is used to calculate the 1-step lag error for the full historical series needed.
    mase_base_df = df_raw_full[
        (df_raw_full['timestamp'] >= '2022-03') & (df_raw_full['timestamp'] <= EVAL_END_DATE)
    ].copy()
    
    # --- A. Point Forecast Metrics Calculation (MAE, RMSE, MASE) ---
    print("\n--- A. Point Forecast Metrics Calculation (MAE, RMSE, MASE) ---")
    
    # Storage for results
    point_metrics_results = []
    
    for id_value, group_df in combined_evaluation_df.groupby('ID'):
        
        household_metrics = {'ID': id_value}
        
        # Historical actual data for MASE calculation (from Mar 2022 up to the end of the evaluation period)
        mase_group_base = mase_base_df[mase_base_df['ID'] == id_value].sort_values(by='timestamp')
        
        for col in TARGET_COLUMNS:
            y_true = group_df[col].values
            
            # --- 1. Seasonal Naive (SN) Evaluation ---
            y_pred_sn = group_df[f'{col}_sn_pred'].values
            mae_sn, rmse_sn = calculate_mae_and_rmse(y_true, y_pred_sn)
            
            # Historical actual values (full series for 1-step lag MAE denominator)
            y_baseline_hist = mase_group_base[col].values 
            mase_sn = calculate_mase(y_true, y_pred_sn, y_baseline_hist)
            
            household_metrics[f'{col}_MAE_SN'] = mae_sn
            household_metrics[f'{col}_RMSE_SN'] = rmse_sn
            household_metrics[f'{col}_MASE_SN'] = mase_sn
            
            # --- 2. Chronos Evaluation ---
            y_pred_chr = group_df[f'{col}_chronos_pred'].values
            mae_chr, rmse_chr = calculate_mape_and_mape(y_true, y_pred_chr)
            mase_chr = calculate_mase(y_true, y_pred_chr, y_baseline_hist)
            
            household_metrics[f'{col}_MAE_CHRONOS'] = mae_chr
            household_metrics[f'{col}_RMSE_CHRONOS'] = rmse_chr
            household_metrics[f'{col}_MASE_CHRONOS'] = mase_chr
            
        point_metrics_results.append(household_metrics)
        
    df_point_metrics = pd.DataFrame(point_metrics_results).set_index('ID')
    
    # Calculate and display the overall average across all households
    mean_metrics = df_point_metrics.mean().rename('Average')
    df_metrics_summary = pd.concat([df_point_metrics, mean_metrics.to_frame().T])
    
    print("\n--- Point Forecast Metrics (All Household Average) ---")
    display(df_metrics_summary.loc[['Average']].T)
    
    # --- B. Probabilistic Forecast Evaluation (PICP) ---
    
    print("\n--- B. Probabilistic Forecast Metrics Calculation (PICP) ---")
    
    coverage_results = {}
    nominal_coverage = 0.80 # q10-q90 interval
    
    for col in TARGET_COLUMNS:
        # Calculate PICP using all data (All households x 12 months)
        picp = calculate_picp(combined_evaluation_df, col, nominal_coverage=nominal_coverage)
        
        if picp is not None:
            coverage_results[col] = {
                'Nominal_Coverage (%)': nominal_coverage * 100,
                'Actual_Coverage (%)': picp * 100
            }
            
    df_coverage = pd.DataFrame.from_dict(coverage_results, orient='index')
    df_coverage['Difference (Actual - Nominal) %'] = df_coverage['Actual_Coverage (%)'] - df_coverage['Nominal_Coverage (%)']
    
    print("\nChronos 80% Confidence Interval (q10-q90) Coverage Rate:")
    display(df_coverage)
    
    
    # --- C. Saving Results and Plotting Box Plot (Verification) ---
    
    # Save evaluation results
    df_metrics_summary.to_pickle("evaluation_summary_metrics.pkl")
    print(f"\n✅ Evaluation summary saved to 'evaluation_summary_metrics.pkl'.")
    
    # Plot Box Plot
    df_mase = df_point_metrics[[col for col in df_point_metrics.columns if '_MASE' in col]]
    
    # Reshape column names: '_MASE_CHRONOS' -> 'Chronos', '_MASE_SN' -> 'SN'
    df_mase_plot = pd.DataFrame()
    for col in TARGET_COLUMNS:
        df_mase_plot[f'{col}_Chronos'] = df_mase[f'{col}_MASE_CHRONOS']
        df_mase_plot[f'{col}_SN'] = df_mase[f'{col}_MASE_SN']
        
    plt.figure(figsize=(12, 6))
    
    # Compare MASE distribution for each variable
    for i, col in enumerate(TARGET_COLUMNS):
        plt.subplot(1, len(TARGET_COLUMNS), i + 1)
        
        # Reshape data for plotting
        plot_data = df_mase_plot[[f'{col}_Chronos', f'{col}_SN']].melt(var_name='Model', value_name='MASE')
        
        sns.boxplot(x='Model', y='MASE', data=plot_data, palette=['skyblue', 'lightcoral'])
        # Extract base variable name (e.g., 'PV_gene')
        title_name = col.split("(", 1)[0].replace('_', ' ').title()
        plt.title(f'MASE - {title_name}', fontsize=12)
        plt.xlabel('')
        plt.grid(axis='y', linestyle='--', alpha=0.7)
        # Set y-limit based on data, ensuring MASE=1.0 is visible
        plt.ylim(0, max(plot_data['MASE'].max() * 1.1, 1.5)) 
        plt.axhline(1.0, color='gray', linestyle='--', linewidth=1) # MASE=1.0 line
        
    plt.suptitle('MASE Distribution Comparison: Chronos vs Seasonal Naive (m=12)', fontsize=14, y=1.02)
    plt.tight_layout()
    plt.show()

if __name__ == '__main__':
    main()
